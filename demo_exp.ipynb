{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, CLIPTextModelWithProjection, AutoProcessor\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import yaml\n",
    "import json\n",
    "from sklearn.svm import LinearSVC\n",
    "import argparse\n",
    "from datasets import load_dataset, Dataset\n",
    "from scipy.optimize import minimize\n",
    "from kneed import KneeLocator\n",
    "from sentence_transformers import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Argument Parser\")\n",
    "# parser.add_argument('--config_path', type=str)\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(\"exp_configs/ff_ff_stereotype_gender.yml\"))\n",
    "# config = yaml.safe_load(open(args.config_path))\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_to_debias = config['att_to_debias']\n",
    "debias_with_ground_truth = config['debias_with_ground_truth']\n",
    "debias_with_predicted_labels = config['debias_with_predicted_labels']\n",
    "reference_dataset_name = config['reference_dataset_name']\n",
    "target_dataset_name = config['target_dataset_name']\n",
    "model_ID = config['model_ID']\n",
    "optimization_method = config['optimization_method']\n",
    "query_type = config['query_type']\n",
    "random_seed = config['random_seed']\n",
    "\n",
    "if query_type == 'hair':\n",
    "    QUERY_TYPE = 'celeba_features'\n",
    "    query_classes = ['Blond_Hair', 'Black_Hair', 'Brown_Hair', 'Gray_Hair']\n",
    "\n",
    "elif query_type == 'stereotype':\n",
    "    QUERY_TYPE = 'criminal_justice'\n",
    "    query_classes = get_query_classes(QUERY_TYPE)\n",
    "else:\n",
    "    raise('query type not implemented')\n",
    "\n",
    "print(f\"query_classes: {query_classes}\")\n",
    "\n",
    "dataset_map = {}\n",
    "dataset_map['CelebaHQ_dialog'] = \"data/celeba_hq_dialog_ff_race.jsonl\"\n",
    "dataset_map['FairFace'] = 'data/openai_clip_cit_large_patch14_fairface_vectorized.jsonl'\n",
    "dataset_map['UTKFace'] = 'data/utk_center_cropped_ff_race.jsonl'\n",
    "\n",
    "reference_ds_path = dataset_map[reference_dataset_name]\n",
    "target_ds_path = dataset_map[target_dataset_name]\n",
    "\n",
    "print(f\"reference_dataset_name: {reference_dataset_name}, reference_ds_path: {reference_ds_path}\")\n",
    "print(f\"target_dataset_name: {target_dataset_name}, target_ds_path: {target_ds_path}\")\n",
    "\n",
    "#\n",
    "normalize = True\n",
    "lam = 1000\n",
    "#\n",
    "\n",
    "if att_to_debias == 'race':\n",
    "    if target_dataset_name == 'UTKFace':\n",
    "        att_elements = ['White', 'Black', 'Asian', 'Indian', 'Latino Hispanic']\n",
    "    else:\n",
    "        att_elements = ['Black', 'East Asian', 'Indian', 'Latino_Hispanic', 'Middle Eastern', 'Southeast Asian', 'White']\n",
    "elif att_to_debias == 'gender':\n",
    "    att_elements = ['Male', 'Female']\n",
    "else:\n",
    "    print('{att_to_debias} not implemented')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_id = 'openai/clip-vit-large-patch14'\n",
    "\n",
    "vl_model = CLIPModel.from_pretrained(model_ID).to(device)\n",
    "processor = AutoProcessor.from_pretrained(model_ID)\n",
    "\n",
    "def get_embeddings(input_text : list, clip_model, clip_processor, normalize=True):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = clip_processor(text=input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        query_text_embedding = clip_model.get_text_features(**inputs)#.to('cpu').numpy()\n",
    "\n",
    "    if normalize:\n",
    "        query_text_embedding /= query_text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return query_text_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = load_dataset(\"json\", data_files=target_ds_path, split='train')\n",
    "embeddings_dataset = embeddings_dataset.with_format(\"np\", columns=[\"embedding\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = embeddings_dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def utk_ethnicity_map(x):\n",
    "    if x == 0:\n",
    "        return 'White'\n",
    "    if x == 1:\n",
    "        return 'Black'\n",
    "    if x == 2:\n",
    "        return 'Asian'\n",
    "    if x == 3:\n",
    "        return 'Indian'\n",
    "    if x == 4:\n",
    "        return 'Latino Hispanic'\n",
    "\n",
    "\n",
    "\n",
    "def load_embedding_dataset(ds_path):\n",
    "    embeddings_dataset = load_dataset(\"json\", data_files=ds_path, split='train')\n",
    "    embeddings_dataset = embeddings_dataset.with_format(\"np\", columns=[\"embedding\"], output_all_columns=True)\n",
    "    for i in range(5):\n",
    "        fold_list = []\n",
    "        for j in range(len(embeddings_dataset)):\n",
    "            if np.random.rand() < 0.9:\n",
    "                fold_list.append(1)\n",
    "            else:\n",
    "                fold_list.append(0)\n",
    "        embeddings_dataset = embeddings_dataset.add_column(f'fold_{i}', fold_list)\n",
    "    if 'Male' in embeddings_dataset.features.keys():\n",
    "        # embeddings_dataset = embeddings_dataset.rename_column('Male', 'gender')\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"gender\": 'Male' if x['Male'] == 1 else 'Female'})\n",
    "    if 'utk_race' in embeddings_dataset.features.keys():\n",
    "        # embeddings_dataset = embeddings_dataset.rename_column('Male', 'gender')\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"race\": utk_ethnicity_map(x['utk_race'])})\n",
    "\n",
    "    if normalize:\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"embedding\": x['embedding'].reshape(-1)/np.linalg.norm(x['embedding'].reshape(-1))})\n",
    "        embeddings_dataset.add_faiss_index(column=\"embedding\")\n",
    "    else:\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"embedding\": x['embedding'].reshape(-1)})\n",
    "        embeddings_dataset.add_faiss_index(column=\"embedding\")\n",
    "    return embeddings_dataset\n",
    "\n",
    "def load_split_embedding_dataset(ds_path, test_split = 0.5):\n",
    "    embeddings_dataset = load_dataset(\"json\", data_files=ds_path, split='train')\n",
    "    embeddings_dataset = embeddings_dataset.with_format(\"np\", columns=[\"embedding\"], output_all_columns=True)\n",
    "    for i in range(5):\n",
    "        fold_list = []\n",
    "        for j in range(len(embeddings_dataset)):\n",
    "            if np.random.rand() < 0.9:\n",
    "                fold_list.append(1)\n",
    "            else:\n",
    "                fold_list.append(0)\n",
    "        embeddings_dataset = embeddings_dataset.add_column(f'fold_{i}', fold_list)\n",
    "    if 'Male' in embeddings_dataset.features.keys():\n",
    "        # embeddings_dataset = embeddings_dataset.rename_column('Male', 'gender')\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"gender\": 'Male' if x['Male'] == 1 else 'Female'})\n",
    "    if 'utk_race' in embeddings_dataset.features.keys():\n",
    "        # embeddings_dataset = embeddings_dataset.rename_column('Male', 'gender')\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"race\": utk_ethnicity_map(x['utk_race'])})\n",
    "    if normalize:\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"embedding\": x['embedding'].reshape(-1)/np.linalg.norm(x['embedding'].reshape(-1))})\n",
    "    else:\n",
    "        embeddings_dataset = embeddings_dataset.map(\n",
    "            lambda x: {\"embedding\": x['embedding'].reshape(-1)})\n",
    "    split = embeddings_dataset.train_test_split(test_size=test_split)\n",
    "    r_embeddings_dataset = split['train']\n",
    "    t_embeddings_dataset = split['test']\n",
    "    r_embeddings_dataset.add_faiss_index(column=\"embedding\")\n",
    "    t_embeddings_dataset.add_faiss_index(column=\"embedding\")\n",
    "    return r_embeddings_dataset, t_embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if reference_dataset_name == target_dataset_name:\n",
    "    reference_embeddings_dataset, target_embeddings_dataset = load_split_embedding_dataset(reference_ds_path, test_split = 0.5)\n",
    "else:\n",
    "    reference_embeddings_dataset = load_embedding_dataset(reference_ds_path)\n",
    "    target_embeddings_dataset = load_embedding_dataset(target_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_neighbors(query_vec, embed_dataset, k = None):\n",
    "    cos_scores = util.cos_sim(query_vec.astype(float), embed_dataset['embedding'].astype(float))\n",
    "    if k is None:\n",
    "        _k = len(embed_dataset)\n",
    "    else:\n",
    "        _k = k\n",
    "    top_results = torch.topk(cos_scores, k=_k)\n",
    "    topk_sim = top_results.values.cpu().numpy().reshape(-1)\n",
    "    top_indices = top_results.indices.cpu().numpy()[0]\n",
    "\n",
    "    if k is None:\n",
    "        kn = KneeLocator([i for i in range(len(topk_sim))], topk_sim, curve='convex', direction='decreasing').knee\n",
    "        print(kn)\n",
    "        top_indices = top_indices[:kn]\n",
    "        topk_sim = topk_sim[:kn]\n",
    "    dist_scores = 1. - topk_sim\n",
    "    neighbors = embed_dataset[top_indices]\n",
    "    \n",
    "    return dist_scores, neighbors\n",
    "\n",
    "def get_embeddings(input_text : list, clip_model, clip_processor, normalize=True):\n",
    "    # tokenized_query_text = clip_tokenizer(input_text, padding=True, return_tensors=\"pt\").to(device)\n",
    "    # with torch.no_grad():\n",
    "    #     query_text_embedding = clip_model(**tokenized_query_text)['text_embeds']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = clip_processor(text=input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "        query_text_embedding = clip_model.get_text_features(**inputs)#.to('cpu').numpy()\n",
    "\n",
    "    if normalize:\n",
    "        query_text_embedding /= query_text_embedding.norm(dim=-1, keepdim=True)\n",
    "    return query_text_embedding\n",
    "\n",
    "def get_proj_matrix(embeddings):\n",
    "    tSVD = TruncatedSVD(n_components=len(embeddings))\n",
    "    embeddings_ = tSVD.fit_transform(embeddings)\n",
    "    basis = tSVD.components_.T\n",
    "\n",
    "    # orthogonal projection\n",
    "    proj = np.linalg.inv(np.matmul(basis.T, basis))\n",
    "    proj = np.matmul(basis, proj)\n",
    "    proj = np.matmul(proj, basis.T)\n",
    "    proj = np.eye(proj.shape[0]) - proj\n",
    "    return proj\n",
    "\n",
    "def get_A(z_i, z_j):\n",
    "    z_i = z_i[:, None]\n",
    "    z_j = z_j[:, None]\n",
    "    return np.matmul(z_i, z_i.T) + np.matmul(z_j, z_j.T) - np.matmul(z_i, z_j.T) - np.matmul(z_j, z_i.T)\n",
    "\n",
    "def get_M(embeddings, S):\n",
    "    d = embeddings.shape[1]\n",
    "    M = np.zeros((d, d))\n",
    "    for s in S:\n",
    "        M  += get_A(embeddings[s[0]], embeddings[s[1]])\n",
    "    return M / len(S)\n",
    "# Define the objective function with additional parameter initial_e\n",
    "def objective(e_star, initial_e):\n",
    "    return float(-np.dot(e_star, initial_e))\n",
    "\n",
    "# Define the constraints with additional parameters f_mean and m_mean\n",
    "def eq_dist_constraint(e_star, y_mean, x_mean):\n",
    "    return float(np.dot(e_star, y_mean) - np.dot(e_star, x_mean))\n",
    "\n",
    "def norm_constraint(e_star):\n",
    "    return float(np.dot(e_star, e_star) - 1)\n",
    "\n",
    "def legrange_text(query_embedding, ref_dataset, spurious_label, spurious_class_list, num_neighbors, proj_matrix, normalize=True):\n",
    "\n",
    "    if proj_matrix is not None:\n",
    "        query_embedding = np.matmul(query_embedding, proj_matrix.T)\n",
    "\n",
    "    if normalize:\n",
    "        norm = np.linalg.norm(query_embedding, axis=-1, keepdims=True)\n",
    "        # print(norm)\n",
    "        query_embedding /= norm\n",
    "    \n",
    "    t_embed = query_embedding.reshape(-1)\n",
    "    q_t = query_embedding.reshape(1,-1)\n",
    "    ref_scores, ref_samples = get_cos_neighbors(query_embedding, ref_dataset, k = len(ref_dataset))\n",
    "\n",
    "    \n",
    "\n",
    "    ref_embed_array = np.asarray(ref_samples['embedding'])\n",
    "    ref_spurious_array = np.asarray(ref_samples[spurious_label])\n",
    "\n",
    "    spurious_anchor_class = spurious_class_list[0]\n",
    "\n",
    "    if num_neighbors is None:\n",
    "        _sim_scores = ref_scores[ref_spurious_array == spurious_anchor_class]\n",
    "        _sim_scores = 1. - _sim_scores\n",
    "        kn = KneeLocator([i for i in range(len(_sim_scores))], _sim_scores, curve='convex', direction='decreasing').knee\n",
    "        s_k = kn\n",
    "        print(kn)\n",
    "    else:\n",
    "        s_k = num_neighbors\n",
    "\n",
    "    anchor_ref_embed_array = ref_embed_array[ref_spurious_array == spurious_anchor_class][:s_k]\n",
    "    # if proj_matrix is not None:\n",
    "    #     anchor_ref_embed_array = np.matmul(anchor_ref_embed_array, proj_matrix.T)  \n",
    "    anchor_prototype = anchor_ref_embed_array.mean(axis=0)\n",
    "\n",
    "    #initial guess \n",
    "    x0 = query_embedding.reshape(-1).astype(float)\n",
    "    # Parameters\n",
    "    x_mean = anchor_prototype.reshape(-1).astype(float)\n",
    "\n",
    "    y_means = []\n",
    "\n",
    "    for spurious_class in spurious_class_list[1:]:\n",
    "        if num_neighbors is None:\n",
    "            _sim_scores = ref_scores[ref_spurious_array == spurious_class]\n",
    "            _sim_scores = 1. - _sim_scores\n",
    "            kn = KneeLocator([i for i in range(len(_sim_scores))], _sim_scores, curve='convex', direction='decreasing').knee\n",
    "            s_k = kn\n",
    "            print(kn)\n",
    "        else:\n",
    "            s_k = num_neighbors\n",
    "        print()\n",
    "        s_ref_embed_array = ref_embed_array[ref_spurious_array == spurious_class][:s_k]\n",
    "        # if proj_matrix is not None:\n",
    "        #     s_ref_embed_array = np.matmul(s_ref_embed_array, proj_matrix.T)       \n",
    "        s_prototype = s_ref_embed_array.mean(axis=0)\n",
    "        y_means.append(s_prototype)\n",
    "        \n",
    "    # Define the constraints in dictionary form with additional parameters\n",
    "    norm_con = {'type': 'eq', 'fun': norm_constraint}\n",
    "    cons = [norm_con]\n",
    "    for _y_mean in y_means:\n",
    "        dist_con = {'type': 'eq', 'fun': eq_dist_constraint, 'args': (_y_mean, x_mean)}\n",
    "        cons.append(dist_con)\n",
    "\n",
    "    solution = minimize(objective, x0, args=(query_embedding.reshape(-1),), method='SLSQP', constraints=cons)\n",
    "    e_star = solution.x\n",
    "    e_star = e_star.reshape(1,-1)\n",
    "\n",
    "    return e_star, x_mean, y_means\n",
    "\n",
    "\n",
    "\n",
    "def log_with_eps(x, eps=1e-10):\n",
    "    if x < eps:\n",
    "        return np.log(eps)\n",
    "    else:\n",
    "        return np.log(x)\n",
    "\n",
    "def max_skew(returned_samples, target_dist, spurious_label='gender', target_classes = [-1,1]):\n",
    "    print(target_dist)\n",
    "    maxskew = 0\n",
    "    for cl in target_classes:\n",
    "        p_y_ds = target_dist[cl]\n",
    "        p_y_returned = (np.asarray(returned_samples[spurious_label])==cl).astype(int).mean()\n",
    "        # print(f\"p_y_returned: {p_y_returned}, p_y_ds: {p_y_ds}\")\n",
    "        candidate_skew = log_with_eps(p_y_returned/p_y_ds)# np.log(p_y_returned/p_y_ds) \n",
    "        if candidate_skew > maxskew:\n",
    "            maxskew = candidate_skew\n",
    "    return maxskew\n",
    "\n",
    "def get_kl(returned_samples, target_dist, spurious_label='gender', target_classes = [-1,1]):\n",
    "    kl = 0\n",
    "    for cl in target_classes:\n",
    "        p_y_ds = target_dist[cl]\n",
    "        p_y_returned = (np.asarray(returned_samples[spurious_label])==cl).astype(int).mean()\n",
    "        # print(f\"p_y_returned: {p_y_returned}, p_y_ds: {p_y_ds}\")\n",
    "        kl += p_y_returned * log_with_eps(p_y_returned/p_y_ds)#np.log(p_y_returned/p_y_ds)\n",
    "    return kl\n",
    "\n",
    "def _cl_with_max_skew(returned_samples, target_dist, spurious_label='gender', target_classes = [-1,1]):\n",
    "    # print(target_dist)\n",
    "    maxskew = 0\n",
    "    max_skew_cl = None\n",
    "\n",
    "    for cl in target_classes:\n",
    "        p_y_ds = target_dist[cl]\n",
    "        p_y_returned = (np.asarray(returned_samples[spurious_label])==cl).astype(int).mean()\n",
    "        # print(f\"p_y_returned: {p_y_returned}, p_y_ds: {p_y_ds}\")\n",
    "        candidate_skew = log_with_eps(p_y_returned/p_y_ds)#np.log(p_y_returned/p_y_ds)\n",
    "        if candidate_skew > maxskew:\n",
    "            maxskew = candidate_skew\n",
    "            max_skew_cl = cl\n",
    "    if max_skew_cl is None:\n",
    "        max_skew_cl = cl\n",
    "    return max_skew_cl\n",
    "\n",
    "def _cl_with_min_skew(returned_samples, target_dist, spurious_label='gender', target_classes = [-1,1]):\n",
    "    # print(target_dist)\n",
    "    minskew = 100000\n",
    "    min_skew_cl = None\n",
    "    for cl in target_classes:\n",
    "        p_y_ds = target_dist[cl]\n",
    "        p_y_returned = (np.asarray(returned_samples[spurious_label])==cl).astype(int).mean()\n",
    "        # print(f\"p_y_returned: {p_y_returned}, p_y_ds: {p_y_ds}\")\n",
    "        candidate_skew = log_with_eps(p_y_returned/p_y_ds)#np.log(p_y_returned/p_y_ds)\n",
    "        if candidate_skew < minskew:\n",
    "            minskew = candidate_skew\n",
    "            min_skew_cl = cl\n",
    "    if min_skew_cl is None:\n",
    "        min_skew_cl = cl\n",
    "    return min_skew_cl\n",
    "\n",
    "def group_accuracy(retrieved_samples, q_class,  spurious_label, spurious_class_list):\n",
    "    s_array = np.asarray(retrieved_samples[spurious_label])\n",
    "    class_array = np.asarray(retrieved_samples[q_class])\n",
    "    result_dict = {}\n",
    "    for s_class in spurious_class_list:\n",
    "        result_dict[s_class] = (class_array[s_array == s_class] == 1).mean()\n",
    "    return result_dict\n",
    "\n",
    "def auc_roc(q_embed, embed_dataset,  q_class, spurious_label, spurious_class_list, metric_is_distance = True):\n",
    "    all_scores, all_samples = get_cos_neighbors(q_embed, embed_dataset, k = len(embed_dataset))\n",
    "    \n",
    "\n",
    "    s_array = np.asarray(all_samples[spurious_label])\n",
    "    print(s_array.shape)\n",
    "    _class_array = np.asarray(all_samples[q_class])\n",
    "    print(_class_array)\n",
    "    binary_class_array = (_class_array== 1 ).astype(int)\n",
    "    # print(binary_class_array)\n",
    "    score_array = np.asarray(all_scores)\n",
    "    if metric_is_distance:\n",
    "        score_array = -1*score_array\n",
    "    # print(score_array.shape)\n",
    "    result_dict = {}\n",
    "    for s_class in spurious_class_list:\n",
    "        s_binary_class_array = binary_class_array[s_array == s_class]\n",
    "        if len(np.unique(s_binary_class_array)) != 1:\n",
    "\n",
    "            s_score_array = score_array[s_array == s_class]\n",
    "            result_dict[s_class] = roc_auc_score(s_binary_class_array, s_score_array)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worst_group_performance(method_metric_dict, higher_better=True):\n",
    "    worst_class = None\n",
    "    if higher_better:\n",
    "        worst_metric = 100000\n",
    "    else: \n",
    "        worst_metric = -100000\n",
    "    for spurious_att in method_metric_dict.keys():  \n",
    "        if  higher_better:\n",
    "            if method_metric_dict[spurious_att] < worst_metric:\n",
    "                worst_metric = method_metric_dict[spurious_att]\n",
    "                worst_class = spurious_att\n",
    "        else:\n",
    "            if method_metric_dict[spurious_att] > worst_metric:\n",
    "                worst_metric = method_metric_dict[spurious_att]\n",
    "                worst_class = spurious_att\n",
    "    return worst_metric, worst_class\n",
    "\n",
    "\n",
    "def get_best_group_performance(method_metric_dict, higher_better=True):\n",
    "    best_class = None\n",
    "    if higher_better:\n",
    "        best_metric = -100000\n",
    "    else: \n",
    "        best_metric = 100000\n",
    "    for spurious_att in method_metric_dict.keys():  \n",
    "        if  higher_better:\n",
    "            if method_metric_dict[spurious_att] > best_metric:\n",
    "                best_metric = method_metric_dict[spurious_att]\n",
    "                best_class = spurious_att\n",
    "        else:\n",
    "            if method_metric_dict[spurious_att] < best_metric:\n",
    "                best_metric = method_metric_dict[spurious_att]\n",
    "                best_class = spurious_att\n",
    "    return best_metric, best_class\n",
    "\n",
    "\n",
    "def relevency(returned_samples, q_class, spurious_label='gender', spurious_class_list = [-1,1]):\n",
    "    result_dict = {}\n",
    "    spurious_label_array = np.asarray(returned_samples[spurious_label])\n",
    "    query_class_array = np.asarray(returned_samples[q_class])\n",
    "    for cl in spurious_class_list: \n",
    "        samples_for_cl = query_class_array[spurious_label_array==cl]\n",
    "        p_rel = (samples_for_cl==1).astype(int).mean()\n",
    "        result_dict[cl] = p_rel\n",
    "    return result_dict\n",
    "\n",
    "def get_metrics(q_embedding, query_class, att_to_debias, K, spurious_att_prior, target_spurious_class_list, target_datasets, name='Vanilla', QUERY_IS_LABELED=True):\n",
    "    result_d = {}\n",
    "    for i in range(5):\n",
    "        _result = {}\n",
    "        _t_ds = target_datasets[i]\n",
    "        \n",
    "        _scores, _samples = get_cos_neighbors(q_embedding, _t_ds, k = K)\n",
    "        # _scores, _samples = target_embeddings_dataset.get_nearest_examples(\n",
    "        # \"embedding\", q_embedding, k=K)\n",
    "        if QUERY_IS_LABELED:\n",
    "            _auc_roc = auc_roc(q_embedding, _t_ds,  query_class, att_to_debias, spurious_class_list = target_spurious_class_list)\n",
    "            worst_metric_val, worst_group = get_worst_group_performance(_auc_roc)\n",
    "            best_metric_val, best_group = get_best_group_performance(_auc_roc)\n",
    "            print(f\"{name} worst group AUC ROC: {worst_metric_val}, worst group: {worst_group}\")\n",
    "            _result['worst_auc_roc_val'] = worst_metric_val\n",
    "            _result['worst_auc_roc_group'] = worst_group\n",
    "\n",
    "            _result['best_auc_roc_val'] = best_metric_val\n",
    "            _result['best_auc_roc_group'] = best_group\n",
    "\n",
    "            print(f\"{name} gap for AUC ROC: {best_metric_val - worst_metric_val}\")\n",
    "            _result['auc_roc_gap'] = best_metric_val - worst_metric_val\n",
    "\n",
    "            _relevency = relevency(_samples,  query_class, att_to_debias, spurious_class_list = target_spurious_class_list)\n",
    "            worst_rel_val, worst_rel_group = get_worst_group_performance(_relevency)\n",
    "            print(f\"{name} worst group relevency: {worst_rel_val}, worst group: {worst_rel_group}\")\n",
    "            _result['worst_rel_val'] = worst_rel_val\n",
    "            _result['worst_rel_group'] = worst_rel_group\n",
    "\n",
    "        max_skew_prior = max_skew(_samples, spurious_att_prior, spurious_label = att_to_debias , target_classes=target_spurious_class_list)\n",
    "        kl_prior = get_kl(_samples, spurious_att_prior, spurious_label = att_to_debias , target_classes=target_spurious_class_list)\n",
    "        print(f\"{name} Max Skew Prior: {max_skew_prior}\")\n",
    "        print(f\"{name} KL Prior: {kl_prior}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        _result['max_skew_prior'] = max_skew_prior\n",
    "        _result['kl_prior'] = kl_prior\n",
    "        result_d[f\"fold_{i}\"] = _result\n",
    "    print()\n",
    "    return result_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_datasets = {}\n",
    "for i in range(5):\n",
    "    t = np.asarray([i for i in range(len(target_embeddings_dataset))])\n",
    "    _t_ds = Dataset.from_dict(target_embeddings_dataset[t[(target_embeddings_dataset[f'fold_{i}'] == 1)]])\n",
    "    _t_ds = _t_ds.with_format(\"np\", columns=[\"embedding\"], output_all_columns=True)\n",
    "    target_datasets[i] = _t_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_type == 'hair':\n",
    "    query_is_labeled = True\n",
    "else:\n",
    "    query_is_labeled = False\n",
    "\n",
    "ref_spurious_class_list = att_elements\n",
    "target_spurious_class_list = att_elements\n",
    "\n",
    "result_dict = {}\n",
    "for query_class in query_classes:\n",
    "    result_dict[query_class] = {}\n",
    "    query_text = [instantiated_search_classes[query_class]['query']]\n",
    "    print(query_class)\n",
    "    print(query_text)\n",
    "\n",
    "\n",
    "    spurious_prompt = instantiated_search_classes[query_class]['spurious_prompts']\n",
    "    inclusive_candidate_prompt = instantiated_search_classes[query_class]['inclusive_candidate_prompts']\n",
    "    exclusive_candidate_prompt = instantiated_search_classes[query_class]['exclusive_candidate_prompts']\n",
    "    S = [[0,1]]\n",
    "\n",
    "    print(spurious_prompt)\n",
    "    print(inclusive_candidate_prompt)\n",
    "    print(exclusive_candidate_prompt)\n",
    "\n",
    "    spurious_att_array = np.asarray(target_embeddings_dataset[att_to_debias])\n",
    "    ref_spurious_att_array = np.asarray(reference_embeddings_dataset[att_to_debias])\n",
    "\n",
    "    spurious_att_prior = {}\n",
    "    for spurious_att in target_spurious_class_list:\n",
    "        spurious_att_prior[spurious_att] = spurious_att_array[spurious_att_array==spurious_att].shape[0]/spurious_att_array.shape[0]\n",
    "    print(f'spurious att prior: {spurious_att_prior}')\n",
    "\n",
    "\n",
    "    ref_spurious_att_prior = {}\n",
    "    for r_spurious_att in ref_spurious_class_list:\n",
    "        ref_spurious_att_prior[r_spurious_att] = ref_spurious_att_array[ref_spurious_att_array==r_spurious_att].shape[0]/ref_spurious_att_array.shape[0]\n",
    "    print(f'ref spurious att prior: {ref_spurious_att_prior}')\n",
    "\n",
    "    if query_is_labeled:\n",
    "        eg_array = np.asarray(target_embeddings_dataset[query_class])\n",
    "        conditional_spurious_att_prior = {}\n",
    "        conditional_spurious_att_array = spurious_att_array[eg_array==1]\n",
    "        for spurious_att in target_spurious_class_list:\n",
    "            conditional_spurious_att_prior[spurious_att] = conditional_spurious_att_array[conditional_spurious_att_array==spurious_att].shape[0]/conditional_spurious_att_array.shape[0]\n",
    "        print(f'conditional spurious att prior: {conditional_spurious_att_prior}')\n",
    "\n",
    "    if query_is_labeled:\n",
    "        prior_for_metric =spurious_att_prior# conditional_spurious_att_prior\n",
    "    else:\n",
    "        prior_for_metric = spurious_att_prior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    query_text_embedding = get_embeddings(query_text, vl_model, processor, normalize).to('cpu').numpy()\n",
    "    spurious_prompt_embedding = get_embeddings(spurious_prompt, vl_model, processor, normalize).to('cpu').numpy()\n",
    "    inclusive_candidate_prompt_embedding = get_embeddings(inclusive_candidate_prompt, vl_model, processor, normalize).to('cpu').numpy()\n",
    "    exclusive_candidate_prompt_embedding = get_embeddings(exclusive_candidate_prompt, vl_model, processor, normalize).to('cpu').numpy()#\n",
    "\n",
    "    P0 = get_proj_matrix(spurious_prompt_embedding)\n",
    "\n",
    "    M = get_M(inclusive_candidate_prompt_embedding, S)\n",
    "    G = lam * M + np.eye(M.shape[0])\n",
    "    inclusive_P_star = np.matmul(P0, np.linalg.inv(G))\n",
    "\n",
    "    M = get_M(exclusive_candidate_prompt_embedding, S)#\n",
    "    G = lam * M + np.eye(M.shape[0])#\n",
    "    exclusive_P_star = np.matmul(P0, np.linalg.inv(G))#\n",
    "\n",
    "    P0_embeddings = np.matmul(query_text_embedding, P0.T)\n",
    "    P0_embeddings = F.normalize(torch.tensor(P0_embeddings), dim=-1).numpy()\n",
    "\n",
    "    inclusive_P_star_embeddings = np.matmul(query_text_embedding, inclusive_P_star.T)\n",
    "    inclusive_P_star_embeddings = F.normalize(torch.tensor(inclusive_P_star_embeddings), dim=-1).numpy()\n",
    "\n",
    "    exclusive_P_star_embeddings = np.matmul(query_text_embedding, exclusive_P_star.T)\n",
    "    exclusive_P_star_embeddings = F.normalize(torch.tensor(exclusive_P_star_embeddings), dim=-1).numpy()\n",
    "\n",
    "\n",
    "\n",
    "    rewrite_pair_list = []\n",
    "    for e_i in range(inclusive_candidate_prompt_embedding.shape[0]):\n",
    "        for e_j in range(e_i+1, inclusive_candidate_prompt_embedding.shape[0]):\n",
    "            rewrite_pair_list.append(((inclusive_candidate_prompt_embedding[e_i] - inclusive_candidate_prompt_embedding[e_j])/2).reshape(1,-1) )\n",
    "\n",
    "\n",
    "    sub_local_embeddings = np.concatenate(rewrite_pair_list)\n",
    "    print(sub_local_embeddings.shape)\n",
    "    sub_local_embeddings = np.concatenate([spurious_prompt_embedding, sub_local_embeddings])\n",
    "    P0_local = get_proj_matrix(sub_local_embeddings)\n",
    "    #\n",
    "    P0_local_embeddings = np.matmul(query_text_embedding, P0_local.T)\n",
    "    P0_local_embeddings = F.normalize(torch.tensor(P0_local_embeddings), dim=-1).numpy()\n",
    "\n",
    "\n",
    "    M = get_M(exclusive_candidate_prompt_embedding, S)#\n",
    "    G = lam * M + np.eye(M.shape[0])#\n",
    "    local_exclusive_P_star = np.matmul(P0_local, np.linalg.inv(G))#\n",
    "\n",
    "\n",
    "    #######\n",
    "\n",
    "    if att_to_debias == 'gender':\n",
    "        num_neighbors = 100#100#500#100 #100 if gender\n",
    "        K = 500\n",
    "    else: \n",
    "        num_neighbors = 10#10#500#100 #100 if gender\n",
    "        K = 500\n",
    "    ref_dataset = reference_embeddings_dataset#target_embeddings_dataset # reference_embeddings_dataset\n",
    "    spurious_class_list = ref_spurious_class_list#['Female','Male']#[-1,1]#['Female','Male']\n",
    "    \n",
    "    target_dist = ref_spurious_att_prior#spurious_att_prior\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    legrange_local_proj_embeddings, x_mean, y_means = legrange_text(query_text_embedding, reference_embeddings_dataset, spurious_label=att_to_debias, \n",
    "                                            spurious_class_list=att_elements, num_neighbors=num_neighbors, proj_matrix = P0_local, normalize=normalize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    result_dict[query_class]['Vanilla'] = get_metrics(query_text_embedding, query_class, att_to_debias, \n",
    "                                             K, prior_for_metric, target_spurious_class_list, name='Vanilla', target_datasets = target_datasets,\n",
    "                                             QUERY_IS_LABELED=query_is_labeled)\n",
    "\n",
    "\n",
    "    result_dict[query_class]['P0'] = get_metrics(P0_embeddings, query_class, att_to_debias, K, prior_for_metric, \n",
    "                                                      target_spurious_class_list, name='P0', target_datasets = target_datasets, QUERY_IS_LABELED=query_is_labeled)\n",
    "\n",
    "    result_dict[query_class]['Inclusive_P0'] = get_metrics(inclusive_P_star_embeddings, query_class, att_to_debias, K, prior_for_metric,\n",
    "                                                  target_spurious_class_list, name='Inclusive_P0', target_datasets = target_datasets, QUERY_IS_LABELED=query_is_labeled)\n",
    "\n",
    "\n",
    "    print('***'*7)\n",
    "    print()\n",
    "    result_dict[query_class]['legrange_local_proj'] = get_metrics(legrange_local_proj_embeddings, query_class, att_to_debias, K, prior_for_metric, \n",
    "                                                           target_spurious_class_list, name='legrange_local_proj', target_datasets = target_datasets, \n",
    "                                                           QUERY_IS_LABELED=query_is_labeled)\n",
    "\n",
    "\n",
    "    print('--'*7)\n",
    "    print()\n",
    "\n",
    "with open(f'results/{reference_dataset_name}_{target_dataset_name}_{query_type}_{att_to_debias}_fixed.json', 'w') as fp:\n",
    "    json.dump(result_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'kl_prior'\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    legrange_local_proj = []\n",
    "    legrange_reg_proj = []\n",
    "    exclusive_P0  = []\n",
    "    Inclusive_P0  = []\n",
    "    P0  = []\n",
    "    Vanilla  = []\n",
    "    for key in result_dict.keys():\n",
    "        legrange_local_proj.append(result_dict[key]['legrange'][f\"fold_{i}\"][METRIC])\n",
    "        legrange_reg_proj.append(result_dict[key]['legrange_reg_local_proj'][f\"fold_{i}\"][METRIC])\n",
    "        exclusive_P0.append(result_dict[key]['exclusive_P0'][f\"fold_{i}\"][METRIC])\n",
    "        Inclusive_P0.append(result_dict[key]['Inclusive_P0'][f\"fold_{i}\"][METRIC])\n",
    "        P0.append(result_dict[key]['P0'][f\"fold_{i}\"][METRIC])\n",
    "        Vanilla.append(result_dict[key]['Vanilla'][f\"fold_{i}\"][METRIC])\n",
    "    print(f\"legrange_local_proj: {np.mean(legrange_local_proj)}\")\n",
    "    print(f\"legrange_reg_local_proj: {np.mean(legrange_reg_proj)}\")\n",
    "    print(f\"exclusive_P0: {np.mean(exclusive_P0)}\")\n",
    "    print(f\"Inclusive_P0: {np.mean(Inclusive_P0)}\")\n",
    "    print(f\"P0: {np.mean(P0)}\")\n",
    "    print(f\"Base CLIP model: {np.mean(Vanilla)}\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "derm_diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
